train:
  device: "cuda:0"
  lr: 0.0004  # learning rate
  weight_decay: 1.5 # weight decay factor for optimizer
  gamma: 0.2  # factor by which to reduce learning rate
  milestones: [10, 14, 16]  # epoch at which to decay learning rate 
  batch_size: 14  # batch size
  num_workers: 24  # number of worker threads for dataloading
  num_epochs: 24  # number of training epochs
  shuffle_dataset: true  # shuffle dataset for each epoch
  download_dataset: false # wether to download the dataset
  
  # Dataset parameters
  train_subset_size: null  # size of training subset. Set to null to use full dataset
  val_subset_size: null  # size of validation subset. Set to null to use full dataset
  
  # Miscellaneous
  plot: false  # whether to plot the outputs and ground truth during validation
  
  # Checkpoint parameters
  checkpoint_hash: null  # the hash of the checkpoint
  checkpoint_epoch: 0 # the epoch of the checkpoint

# must be divisible by 4 becouse of the Twins model architecture
# twins model was trained on:
# >>> config {'input_size': (3, 224, 224), 'interpolation': 'bicubic', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225), 'crop_pct': 0.9, 'crop_mode': 'center'}
sat_dataset:
  mean: [0.485, 0.456, 0.406] 
  std: [0.229, 0.224, 0.225]
  patch_w: 512
  patch_h: 512
  zoom_level: 16 # satellite dataset zoom level
  root_dir: "./sat"
  kernel_size: 60 # kernel size for the hanning window

drone_dataset:
  mean: [0.485, 0.456, 0.406] 
  std: [0.229, 0.224, 0.225]
  patch_w: 128
  patch_h: 128
  root_dir: "./drone"
  rotation_deg: 90 # 360 means no rotation, 180 means 2 rotations, etc.

val:
  device: "cuda:0"
  checkpoint_hash:  null  # the hash of the checkpoint
  checkpoint_epoch: 23
  val_subset_size: null  # size of validation subset. Set to null to use full dataset
  batch_size: 12  # batch size
  num_workers: 24  # number of worker threads for dataloading
  download_dataset: false # wether to download the dataset
  plot: true 
