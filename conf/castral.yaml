train:
  device: "cuda:0"
  lr_fusion: 0.0004 # learning rate for the fusion part of the model
  lr_backbone: 0.0001 # learning rate for the backbone part of the model
  gamma: 0.2 # factor by which to reduce learning rate -> lr = lr * gamma (on scheduler step)
  milestones: [9, 13, 15] # epoch at which to decay learning rate
  num_workers: 16 # number of worker threads for dataloading
  num_epochs: 24 # number of training epochs
  shuffle_dataset: true # shuffle dataset for each epoch
  download_dataset: false # wether to download the dataset
  batch_size: 4
  train_until_convergence: false # train until convergence (ignore milestones and num_epochs)

  loss_fn:
    "hanning" # or "mse"


    # Dataset parameters
  train_subset_size: 100 # size of training subset. Set to null to use full dataset
  val_subset_size: 50 # size of validation subset. Set to null to use full dataset

  # Miscellaneous
  plot: true # whether to plot the outputs and ground truth during validation

  # Checkpoint parameters
  checkpoint_hash: null # the hash of the checkpoint
  checkpoint_epoch: null # the epoch of the checkpoint

# must be divisible by 4 becouse of the Twins model architecture
# twins model was trained on:
# >>> config {'input_size': (3, 224, 224), 'interpolation': 'bicubic', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225), 'crop_pct': 0.9, 'crop_mode': 'center'}

dataset:
  mean: [0.485, 0.456, 0.406]
  std:
    [0.229, 0.224, 0.225]
  drone_scales: [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0] # drone view scales during training
  #drone_scales: [1.0] # drone view scales during the run
  tiffs: [16]
  drone_patch_w: 128
  drone_patch_h: 128
  sat_zoom_level: 16 # satellite dataset zoom level
  heatmap_kernel_size: 33
  sat_patch_w: 400
  sat_patch_h: 400
  root_dir: "./castral_dataset"
  heatmap_type:
    "hanning" # square (to use with hanning loss function)
    # gaussian (to use with mse loss function)
    # hanning (to use with mse loss function)

val:
  device: "cuda:0"
  checkpoint_hash: "7d2a3e9041eabdb084b818dba32124f94ffc3c9d" # the hash of the checkpoint
  checkpoint_epoch: 23
  val_subset_size: null # size of validation subset. Set to null to use full dataset
  num_workers: 16 # number of worker threads for dataloading
  shuffle_dataset: false # shuffle dataset for each epoch
  batch_size: 6
  download_dataset: false # wether to download the dataset
  plot: true

run:
  device: "cuda:0"
  checkpoint_hash: "b6af30dfbe8d4ed9192df0871146e6f6ba773711" # the hash of the checkpoint
  checkpoint_epoch: 23
  run_subset_size: null # size of validation subset. Set to null to use full dataset
  num_workers: 16 # number of worker threads for dataloading
  batch_size: 1
